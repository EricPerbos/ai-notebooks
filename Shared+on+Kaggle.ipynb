{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple 4-feature Neural Network for LB .547"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few months ago I started trying to apply the techniques Jeremy Howard shares [here](https://github.com/fastai/courses/blob/master/deeplearning2/rossman.ipynb) for making predictions using simple feed-forward neural networks and embedding layers to this contest.\n",
    "\n",
    "It is based on the [this paper](https://arxiv.org/abs/1604.06737) by Guo and Berkhahn who finished 3rd place in the [Rossman Store Sales](https://www.kaggle.com/c/rossmann-store-sales) contest.\n",
    "\n",
    "Well ... it didn't work very well. Here is a quick summary of what I tried along with a simple approach which got me the best results.\n",
    "\n",
    "#### Feel free to ask questions or share any advice in the comments.\n",
    "I'm presuming a lot of knowledge about neural networks. If you take the time to carefully read through my results I'll be happy to share what I know.\n",
    "\n",
    "## What didn't work - a massive, feature-rich dataset\n",
    "I started by systematically building features with all data provided in the contest following the techniques Jeremy Howard outlines in the link above. \n",
    "\n",
    "I generated over 27 category features (e.g. year, month, dayofweek, family, holidays) and 15 continuous features (e.g. oil price, 30-day oil change, days before/after promotion). \n",
    "\n",
    "Some of these were quite complex such as days before/after a promotion given that each depends on a store and item.\n",
    "\n",
    "And I did this for **all of the training data**, resulting in dataframes > 20 GB.\n",
    "\n",
    "Then I built a sophisticated embedding layer and a few dense layers for my model. It took several hours to train on a GTX1080Ti ... and my results were pretty lousy.\n",
    "\n",
    "While it's always possible I made a mistake, I couldn't get anything below 1.0 on the public leaderboard. So I decided to scrap it and try another approach.\n",
    "\n",
    "## What worked better - 4 features and a few months of training data\n",
    "I decided to start afresh trying replicate the results using a handful of features and less data.\n",
    "\n",
    "I started with the 'moving average' technique presnted by Paulo Pinto and others [here](https://www.kaggle.com/paulorzp/log-ma-and-days-of-week-means-lb-0-529/code) and tuned the neural network to get a LB score of .547. The 4 features are:\n",
    "\n",
    "+ store number\n",
    "+ item number\n",
    "+ whether or the item was on promotion\n",
    "+ an average of recent sales at the same store/item combinations.\n",
    "\n",
    "I used some unorthodox technniques (no dropout, batch sizes of 500,000) which I shared below.\n",
    "\n",
    "I got better results and after trying to add lots of additional features never improved on these results.\n",
    "\n",
    "## What I would like to try (but probably won't)\n",
    "This contest was a good reminder to start with simple models, small amounts of training data and gradually add features. \n",
    "\n",
    "My goal was to learn the techniques presented in the Guo paper for using embedding layers on time-series data. I definitely achieved it and now have a good understanding of the approach.\n",
    "\n",
    "#### more features\n",
    "I'd be curious to see if adding oil, weather, holiday, geographical etc. information improves results but I'm not super-optimistic about getting a big boost.\n",
    "\n",
    "#### custom sample weight and loss fucntions\n",
    "On the model side I'd like to figure out why using Keras sample_weight field isn't working and would like to build a custom loss functions with the NWRMSLE instead of MAE. Unfortunately building custom loss functions in Keras gets quite complex using the Keras backend. Keras is better for out-of-the-box functions.\n",
    "\n",
    "#### A good reminder to start simple\n",
    "Unfortunately most of the data in this contest doesn't appear very predictive. In this case a simpler algorithm like xgboost is probably a better choice.\n",
    "\n",
    "Obviously ... we would never tackle a client project using this approach. Start simple, learn the data, and get a good baseline is always the right approach.\n",
    "\n",
    "Hope it helps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "kevin loaded\n"
     ]
    }
   ],
   "source": [
    "#Select between 2 GTX1080Ti GPUs\n",
    "gpu=1\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=$gpu\n",
    "# libraries and utilities\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "#file kevin has most of the standard libraries: numpy, pandas, keras ...\n",
    "import kevin; importlib.reload(kevin)\n",
    "from kevin import *\n",
    "from datetime import timedelta\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#paths\n",
    "slowdata = data_path + 'slowdisk/'; slowdata\n",
    "simple = slowdata + 'simple2/'\n",
    "#leaves duplicate field names on left and appends with _y on right\n",
    "def join_df(left, right, left_on, right_on=None):\n",
    "    if right_on is None: right_on=left_on\n",
    "    return left.merge(right, how='left', left_on=left_on, right_on=right_on, suffixes=(\"\", \"_y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify datatypes before loading the data will save you a ton of memory.\n",
    "dtypes = {'id': np.uint32,\n",
    "          'store_nbr': np.uint8, \n",
    "          'item_nbr': np.uint32, \n",
    "          'unit_sales': np.float32,\n",
    "          'class': np.uint16,\n",
    "          'dcoilwtico':np.float16,\n",
    "          'transactions':np.uint16,\n",
    "          'cluster': np.uint32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(slowdata + 'train.csv', dtype=dtypes, usecols=[1,2,3,4,5], parse_dates=['date'],\n",
    "                    skiprows=range(1, 86672217) #Skip dates before 2016-08-01, and id column\n",
    "                   )\n",
    "test = pd.read_csv(slowdata + 'test.csv', dtype=dtypes, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data older than march 1 showed no improvement\n",
    "train = train[train['date'] >= '2017-03-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['unit_sales'] = train['unit_sales'].as_matrix().clip(min=0) # get rid of negs\n",
    "train['unit_sales'] =  train['unit_sales'].apply(pd.np.log1p) # transform for smoother training\n",
    "train['onpromotion'].fillna(False, inplace=True) # fill in the missing promotion data with 'False'\n",
    "test['onpromotion'].fillna(False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add 'day of the week' to data\n",
    "train['dow'] = train['date'].dt.dayofweek.astype(np.int8)\n",
    "test['dow'] = test['date'].dt.dayofweek.astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_pickle(simple + 'train_processed.pkl')\n",
    "test.to_pickle(simple + 'test_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle(simple + 'train_processed.pkl')\n",
    "test = pd.read_pickle(simple + 'test_processed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive day-of-the-week dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is technique for deriving the key feature for the training algorithm. Before diving into the code consider an example:\n",
    "\n",
    "#### Example\n",
    "Suppose you want to estimate the sale of Beer at your local 7-11.\n",
    "\n",
    "You could just calculate the average amount of (recent) daily sales of Beer at the 7-11. For instance, we may know that 7-11 sells an average of 10 cases a beer every day. So we could just use '10' as the prediction for beer sales.\n",
    "\n",
    "But as it turns out Beer doesn't sell the same every day. Customers buy more Beer on Friday and Saturday and less on Sunday.\n",
    "\n",
    "madw/mawk corrects for this daily factor. After applying this factor we submit a score which estimates 8 for Tues-Thurs, 4 for Sunday, 21 for Friday and Saturday. Average is still 10 but we've scaled for the store and item number.\n",
    "\n",
    "#### Confused? Calculate for 1 store\n",
    "I was initially confused by this technique. After I calculate the same results for 1 store it was clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not really a \"moving average\"\n",
    "I kept the syntax for readability, but this isn't a 'moving' average. It is a daily average by store and item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/paulorzp/log-ma-and-days-of-week-means-lb-0-529/code and others\n",
    "ma_dw = train[['item_nbr','store_nbr','dow','unit_sales']].groupby(\n",
    "        ['item_nbr','store_nbr','dow'])['unit_sales'].mean().to_frame('madw').reset_index()\n",
    "ma_wk = ma_dw[['item_nbr','store_nbr','madw']].groupby(\n",
    "        ['item_nbr', 'store_nbr'])['madw'].mean().to_frame('mawk').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>dow</th>\n",
       "      <th>madw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96995</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96995</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.794513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96995</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.943827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_nbr  store_nbr  dow      madw\n",
       "0     96995          1    0  0.895880\n",
       "1     96995          1    1  0.693147\n",
       "2     96995          1    2  0.693147\n",
       "3     96995          1    3  0.794513\n",
       "4     96995          1    4  0.943827"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember the madw and mawk sales have been transformed by log1p above\n",
    "ma_dw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>mawk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96995</td>\n",
       "      <td>1</td>\n",
       "      <td>0.844024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96995</td>\n",
       "      <td>2</td>\n",
       "      <td>0.775311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96995</td>\n",
       "      <td>3</td>\n",
       "      <td>0.786271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96995</td>\n",
       "      <td>4</td>\n",
       "      <td>0.783251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96995</td>\n",
       "      <td>5</td>\n",
       "      <td>0.882478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_nbr  store_nbr      mawk\n",
       "0     96995          1  0.844024\n",
       "1     96995          2  0.775311\n",
       "2     96995          3  0.786271\n",
       "3     96995          4  0.783251\n",
       "4     96995          5  0.882478"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_wk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create multi-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expanding the training data to list a store/item combination on every data and filling zeros\n",
    "# where no data is listed.\n",
    "# I'm not totally comfortable with this approach because it presumes missing data means no sales.\n",
    "# however, I did get better results by doing it, so I continued.\n",
    "\n",
    "u_dates = train.date.unique()\n",
    "u_stores = train.store_nbr.unique()\n",
    "u_items = train.item_nbr.unique()\n",
    "train.set_index(['date', 'store_nbr', 'item_nbr'], inplace=True)\n",
    "train=train.reindex(pd.MultiIndex.from_product((u_dates, u_stores, u_items),\n",
    "            names = ['date', 'store_nbr', 'item_nbr'])).reset_index()\n",
    "train.unit_sales.fillna(0, inplace=True)\n",
    "train.onpromotion.fillna(0, inplace=True)\n",
    "del u_dates, u_stores, u_items # save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Moving' averages\n",
    "More correctly stated as static revent averages over a time window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lastdate = train.iloc[train.shape[0]-1].date\n",
    "# Only necessary to create dataframe for the loop below. We copy over 'mais' with median later.\n",
    "ma_is = train[['item_nbr','store_nbr','unit_sales']].groupby(\n",
    "    ['item_nbr','store_nbr'])['unit_sales'].mean().to_frame('mais')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mais</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_nbr</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">96995</th>\n",
       "      <th>1</th>\n",
       "      <td>0.128293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.156473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.216649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.092171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mais\n",
       "item_nbr store_nbr          \n",
       "96995    1          0.128293\n",
       "         2          0.156473\n",
       "         3          0.216649\n",
       "         4          0.062589\n",
       "         5          0.092171"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_is.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate average sales of store/item combinations over recent time windows \n",
    "# based on days before the last available training data date.\n",
    "# e.g. 112 days before the last date, 56 before the last date ...\n",
    "for i in [112, 56, 28, 14, 7, 3, 1]:\n",
    "    tmp = train[train.date>lastdate-timedelta(int(i))]\n",
    "    tmpg = tmp.groupby(['item_nbr', 'store_nbr'])['unit_sales'].mean().to_frame('mais'+str(i))\n",
    "    ma_is =  ma_is.join(tmpg, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del tmp; del tmpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now take the median of of the day windows we calculated.\n",
    "# I tried just keeping all of the windows but got worse results.\n",
    "# Perhaps mean would work as well as median - I didn't test it.\n",
    "# In any case, this is a pretty simplistic feature which may not generalize.\n",
    "ma_is['mais']=ma_is.median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mais</th>\n",
       "      <th>mais112</th>\n",
       "      <th>mais56</th>\n",
       "      <th>mais28</th>\n",
       "      <th>mais14</th>\n",
       "      <th>mais7</th>\n",
       "      <th>mais3</th>\n",
       "      <th>mais1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_nbr</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">96995</th>\n",
       "      <th>1</th>\n",
       "      <td>0.141274</td>\n",
       "      <td>0.154255</td>\n",
       "      <td>0.172356</td>\n",
       "      <td>0.295202</td>\n",
       "      <td>0.334438</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.024755</td>\n",
       "      <td>0.161961</td>\n",
       "      <td>0.123776</td>\n",
       "      <td>0.049511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.355917</td>\n",
       "      <td>0.208903</td>\n",
       "      <td>0.286789</td>\n",
       "      <td>0.336299</td>\n",
       "      <td>0.375535</td>\n",
       "      <td>0.454008</td>\n",
       "      <td>0.462098</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.124828</td>\n",
       "      <td>0.093884</td>\n",
       "      <td>0.150635</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.198042</td>\n",
       "      <td>0.231049</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.118639</td>\n",
       "      <td>0.138257</td>\n",
       "      <td>0.202249</td>\n",
       "      <td>0.237278</td>\n",
       "      <td>0.099021</td>\n",
       "      <td>0.198042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mais   mais112    mais56    mais28    mais14  \\\n",
       "item_nbr store_nbr                                                     \n",
       "96995    1          0.141274  0.154255  0.172356  0.295202  0.334438   \n",
       "         2          0.024755  0.161961  0.123776  0.049511  0.000000   \n",
       "         3          0.355917  0.208903  0.286789  0.336299  0.375535   \n",
       "         4          0.124828  0.093884  0.150635  0.099021  0.099021   \n",
       "         5          0.118639  0.138257  0.202249  0.237278  0.099021   \n",
       "\n",
       "                       mais7     mais3     mais1  \n",
       "item_nbr store_nbr                                \n",
       "96995    1          0.099021  0.000000  0.000000  \n",
       "         2          0.000000  0.000000  0.000000  \n",
       "         3          0.454008  0.462098  0.693147  \n",
       "         4          0.198042  0.231049  0.693147  \n",
       "         5          0.198042  0.000000  0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_is.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now remove the mais112 ... mais1 and merge the mais feature with training dataframe\n",
    "ma_is.reset_index(inplace=True)\n",
    "ma_is.drop(list(ma_is.columns.values)[3:],1,inplace=True)\n",
    "train=join_df(train,ma_is,['item_nbr', 'store_nbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>mais</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96995</td>\n",
       "      <td>1</td>\n",
       "      <td>0.141274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96995</td>\n",
       "      <td>2</td>\n",
       "      <td>0.024755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96995</td>\n",
       "      <td>3</td>\n",
       "      <td>0.355917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96995</td>\n",
       "      <td>4</td>\n",
       "      <td>0.124828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96995</td>\n",
       "      <td>5</td>\n",
       "      <td>0.118639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_nbr  store_nbr      mais\n",
       "0     96995          1  0.141274\n",
       "1     96995          2  0.024755\n",
       "2     96995          3  0.355917\n",
       "3     96995          4  0.124828\n",
       "4     96995          5  0.118639"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_is.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the average sales by dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now you see the purpose of 'madw' and 'mawk'. To scale each sales average by\n",
    "# how well it historically sells for that day of the week\n",
    "train = pd.merge(train, ma_wk, how='left', on=['item_nbr','store_nbr'])\n",
    "train = pd.merge(train, ma_dw, how='left', on=['item_nbr','store_nbr','dow'])\n",
    "train['m_ratio'] = train['mais']*train['madw']/train['mawk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### m_ratio is the key feature\n",
    "Lots of work and data wrangling to get to that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 36278928 entries, 0 to 36278927\n",
      "Data columns (total 10 columns):\n",
      "date           datetime64[ns]\n",
      "store_nbr      uint64\n",
      "item_nbr       uint64\n",
      "unit_sales     float32\n",
      "onpromotion    object\n",
      "dow            object\n",
      "mais           float64\n",
      "mawk           float32\n",
      "madw           float32\n",
      "m_ratio        float64\n",
      "dtypes: datetime64[ns](1), float32(3), float64(2), object(2), uint64(2)\n",
      "memory usage: 2.6+ GB\n"
     ]
    }
   ],
   "source": [
    "# yikes, this is big. let's clean it up.\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['store_nbr'] = train['store_nbr'].astype(np.uint8)\n",
    "train['onpromotion'] = train['onpromotion'].astype(np.uint8)\n",
    "train['item_nbr'] = train['item_nbr'].astype(np.uint32)\n",
    "# I reversed the sales normalization here. I apply a different \n",
    "# normalization function before training below.\n",
    "# I didn't test but expect both to give similar results.\n",
    "train['unit_sales'] = train['unit_sales'].apply(np.expm1)\n",
    "train.drop('dow', 1, inplace=True)\n",
    "train.drop('mawk', 1, inplace=True)\n",
    "train.drop('madw', 1, inplace=True)\n",
    "train.drop('mais', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 36278928 entries, 0 to 36278927\n",
      "Data columns (total 6 columns):\n",
      "date           datetime64[ns]\n",
      "store_nbr      uint8\n",
      "item_nbr       uint32\n",
      "unit_sales     float32\n",
      "onpromotion    uint8\n",
      "m_ratio        float64\n",
      "dtypes: datetime64[ns](1), float32(1), float64(1), uint32(1), uint8(2)\n",
      "memory usage: 1.1 GB\n"
     ]
    }
   ],
   "source": [
    "# Now smaller\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now prep the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.merge(test, ma_is, how='left', on=['item_nbr','store_nbr'])\n",
    "test = pd.merge(test, ma_wk, how='left', on=['item_nbr','store_nbr'])\n",
    "test = pd.merge(test, ma_dw, how='left', on=['item_nbr','store_nbr','dow'])\n",
    "test['m_ratio'] = test['mais']*test['madw']/test['mawk']\n",
    "test['store_nbr'] = test['store_nbr'].astype(np.uint8)\n",
    "test['onpromotion'] = test['onpromotion'].astype(np.uint8)\n",
    "test['item_nbr'] = test['item_nbr'].astype(np.uint32)\n",
    "test.drop('id', 1, inplace=True)\n",
    "test.drop('dow', 1, inplace=True)\n",
    "test.drop('mawk', 1, inplace=True)\n",
    "test.drop('madw', 1, inplace=True)\n",
    "test.drop('mais', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del ma_is; del ma_wk; del ma_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up and rescale m_ratio, applying the same denormalization we did to unit sales.\n",
    "# probably makes no difference but didn't test it.\n",
    "for m in ['m_ratio']:\n",
    "    train[m].fillna(0, inplace=True)\n",
    "    test[m].fillna(0, inplace=True)\n",
    "    train[m] = train[m].apply(np.expm1) # probably unnecessary\n",
    "    test[m] = test[m].apply(np.expm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NN features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to scale the continuous variables (in this case there is only one, m_ratio) and turn the category variables to discrete categories.\n",
    "\n",
    "I'm following the technique demonstrated by Jeremy Howard from the Rossman contest, https://github.com/fastai/courses/blob/master/deeplearning2/rossman.ipynb\n",
    "\n",
    "However ... this is overkill for such a small number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used to make the embedding layer.\n",
    "cat_var_dict = {'item_nbr': 5,\n",
    "                'onpromotion': 3,\n",
    "                'store_nbr': 3,\n",
    "               }\n",
    "# time-saver if you have a lot of categories. Overkill here.\n",
    "cat_vars = [o[0] for o in \n",
    "            sorted(cat_var_dict.items(), key=lambda x: x[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['item_nbr', 'onpromotion', 'store_nbr']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just 1!\n",
    "contin_vars = ['m_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LabelEncoder and StandardScaler with DataFrameMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, overkill for this small example but this code really simplifies when you have dozens of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameMapper takes a list of dataframe columns and sklearn transformations.\n",
    "# Seems a bit weird but check the DataFrameMapper docs.\n",
    "# First create a list for the categories and columns.\n",
    "cat_maps = [(o, LabelEncoder()) for o in cat_vars]\n",
    "contin_maps = [([o], StandardScaler()) for o in contin_vars]\n",
    "#Now use DataFrameMapper to apply LabelEncoder to the category variables and StandardScaler\n",
    "#to the continuous ones.\n",
    "cat_mapper = DataFrameMapper(cat_maps)\n",
    "contin_mapper = DataFrameMapper(contin_maps)\n",
    "# Now call ithe fit function in DataFrameMapper\n",
    "# need to append the test data to make sure we account for items, stores not in training data.\n",
    "cat_map_fit = cat_mapper.fit(train.append(test))\n",
    "contin_map_fit = contin_mapper.fit(train.append(test))\n",
    "\n",
    "#we can now apply the DataFrameMapper 'transform' function to transform the data.\n",
    "#we do this in the cat_preproc and contin_preproc functions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the last 10% of data for validation.\n",
    "train_ratio = 0.90\n",
    "train_size = int(train_ratio*len(train))\n",
    "trn = train[:train_size]\n",
    "vld = train[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions to use to make the category and continuous variables\n",
    "def cat_preproc(dat):\n",
    "    return cat_map_fit.transform(dat).astype(np.int16)\n",
    "def contin_preproc(dat):\n",
    "    return contin_map_fit.transform(dat).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now make the input features \n",
    "cat_map_train = cat_preproc(trn)\n",
    "cat_map_valid = cat_preproc(vld)\n",
    "cat_map_test = cat_preproc(test)\n",
    "\n",
    "contin_map_train = contin_preproc(trn)\n",
    "contin_map_valid = contin_preproc(vld)\n",
    "contin_map_test = contin_preproc(test)\n",
    "\n",
    "#for use later\n",
    "contin_cols=contin_map_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the model data\n",
    "pickle.dump(contin_map_fit, open(simple + 'contin_maps.pickle', 'wb'))\n",
    "pickle.dump(cat_map_fit, open(simple + 'cat_maps.pickle', 'wb'))\n",
    "pickle.dump(cat_var_dict, open(simple + 'cat_var_dict.pkl', 'wb'))\n",
    "pickle.dump(cat_map_train, open(simple + 'cat_map_train.pkl', 'wb'), protocol=4)\n",
    "pickle.dump(cat_map_valid, open(simple + 'cat_map_valid.pkl', 'wb'))\n",
    "pickle.dump(contin_map_train, open(simple + 'contin_map_train.pkl', 'wb'), protocol=4)\n",
    "pickle.dump(contin_map_valid, open(simple + 'contin_map_valid.pkl', 'wb'))\n",
    "pickle.dump(cat_map_test, open(simple + 'cat_map_test.pkl', 'wb'))\n",
    "pickle.dump(contin_map_test, open(simple + 'contin_map_test.pkl', 'wb'))\n",
    "pickle.dump(contin_cols, open(simple + 'contin_cols.pkl', 'wb'))\n",
    "trn.to_pickle(simple + 'trn.pkl')\n",
    "vld.to_pickle(simple + 'vld.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_map_train = pickle.load(open(simple + 'cat_map_train.pkl', 'rb'))\n",
    "cat_map_valid = pickle.load(open(simple + 'cat_map_valid.pkl', 'rb'))\n",
    "contin_map_train = pickle.load(open(simple + 'contin_map_train.pkl', 'rb'))\n",
    "contin_map_valid = pickle.load(open(simple + 'contin_map_valid.pkl', 'rb'))\n",
    "cat_map_fit = pickle.load(open(simple + 'cat_maps.pickle', 'rb'))\n",
    "contin_map_fit = pickle.load(open(simple + 'contin_maps.pickle', 'rb'))\n",
    "cat_var_dict = pickle.load(open(simple + 'cat_var_dict.pkl', 'rb'))\n",
    "cat_map_test = pickle.load(open(simple + 'cat_map_test.pkl', 'rb'))\n",
    "contin_map_test = pickle.load(open(simple + 'contin_map_test.pkl', 'rb'))\n",
    "contin_cols = pickle.load(open(simple + 'contin_cols.pkl', 'rb'))\n",
    "trn = pd.read_pickle(simple + 'trn.pkl')\n",
    "vld = pd.read_pickle(simple + 'vld.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of (rows,1) arrays for training\n",
    "def split_cols(arr): return np.hsplit(arr,arr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just some simple array manipulations to prep it for the model.\n",
    "map_train = split_cols(cat_map_train) + [contin_map_train]\n",
    "map_valid = split_cols(cat_map_valid) + [contin_map_valid]\n",
    "map_test = split_cols(cat_map_test) + [contin_map_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normy(y):\n",
    "    meany=np.mean(y)\n",
    "    stdevy = np.std(y)\n",
    "    ynorm = (y-meany)/stdevy\n",
    "    return meany, stdevy, ynorm\n",
    "\n",
    "def denorm(y, meany, stdevy, r=3):\n",
    "    return np.round(y*stdevy+meany, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weighting samples using keras sample_weight made results worse.\n",
    "sample_weight = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#meany_ and stdevy_ used for denorm function\n",
    "meany_train, stdevy_train, y_train = normy(trn.as_matrix(columns = ['unit_sales']))\n",
    "meany_valid, stdevy_valid, y_valid = normy(vld.as_matrix(columns = ['unit_sales']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generates embedding layer for categorical variables\n",
    "#returns input tensor for creating model with Keras model function\n",
    "def get_emb(name, n_in, n_out, reg, shape=1):\n",
    "    inp = Input(shape=(shape,), name=name+'_in')\n",
    "    em = Embedding(n_in, n_out, input_length=shape, W_regularizer=l2(reg))(inp)\n",
    "    em = Flatten(name=name+'_flt')(em)\n",
    "    return inp, em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def cat_input_dim(feat): return len(feat[1].classes_)\n",
    "def cat_input_name(feat): return feat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#continuous input\n",
    "contin_inp = Input((contin_cols,), name='contin')\n",
    "contin_dense = Dense(contin_cols*2, activation='relu', name='contin_dense')(contin_inp)\n",
    "\n",
    "#category embeddings\n",
    "embs = [get_emb(name=cat_input_name(feat), n_in=cat_input_dim(feat),\n",
    "                n_out=cat_var_dict[cat_input_name(feat)], reg=1e-4) \n",
    "        for feat in cat_map_fit.features]\n",
    "\n",
    "x = merge([em for _,em in embs] + [contin_dense], mode='concat')\n",
    "# x = Dropout(0.05)(x) with so few Dense layers dropout was unecessary\n",
    "x = Dense(10, activation='relu', init='uniform')(x)\n",
    "x = BatchNormalization()(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "x = Dense(5, activation='relu', init='uniform')(x)\n",
    "x = BatchNormalization()(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "model = Model([inp for inp,_ in embs] + [contin_inp], x)\n",
    "lr = 1e-1\n",
    "model.compile(optimizer=Adam(lr=lr), loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "item_nbr_in (InputLayer)         (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "onpromotion_in (InputLayer)      (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "store_nbr_in (InputLayer)        (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1, 5)          20295       item_nbr_in[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1, 3)          6           onpromotion_in[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 1, 3)          162         store_nbr_in[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "contin (InputLayer)              (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "item_nbr_flt (Flatten)           (None, 5)             0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "onpromotion_flt (Flatten)        (None, 3)             0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "store_nbr_flt (Flatten)          (None, 3)             0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "contin_dense (Dense)             (None, 2)             4           contin[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 13)            0           item_nbr_flt[0][0]               \n",
      "                                                                   onpromotion_flt[0][0]            \n",
      "                                                                   store_nbr_flt[0][0]              \n",
      "                                                                   contin_dense[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            140         merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 10)            40          dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 5)             55          batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 5)             20          dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             6           batchnormalization_2[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 20,728\n",
      "Trainable params: 20,698\n",
      "Non-trainable params: 30\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#note that most of the features are in the item number embedding layer - an unforunate\n",
    "#consequence of having so few features.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to allow me easier retraining to test model development and change learning \n",
    "#rate on the fly\n",
    "def retrain(model=model, map_train=map_train, y_train=y_train, sample_weight=sample_weight, map_valid=map_valid, y_valid=y_valid,\n",
    "            epochs=[10,10,10], learns = [1e-2, 1e-3, 1e-4], batch_size=5096):\n",
    "    hist=[]\n",
    "    loops = len(epochs)\n",
    "    if len(learns) != loops:\n",
    "        raise ValueError('number of epochs much match number of learning rates')\n",
    "    for l in range(loops):\n",
    "        K.set_value(model.optimizer.lr, learns[l])\n",
    "        history = model.fit(map_train, y_train, batch_size=batch_size, sample_weight=sample_weight,\n",
    "                            nb_epoch=epochs[l], validation_data=(map_valid, y_valid))\n",
    "        hist.append(history.history)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32651035 samples, validate on 3627893 samples\n",
      "Epoch 1/1\n",
      "32651035/32651035 [==============================] - 20s - loss: 0.1615 - val_loss: 1.4143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f090b48eeb8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yes ... this looks like an insanely high batch size.\n",
    "#each training example is very small.\n",
    "#obviously can't do this with images!\n",
    "#The model trained faster and was model stable with large batches.\n",
    "model.fit(map_train, y_train, batch_size=500000, nb_epoch=1, validation_data=(map_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32651035 samples, validate on 3627893 samples\n",
      "Epoch 1/1\n",
      "32651035/32651035 [==============================] - 22s - loss: 0.1121 - val_loss: 0.1338\n",
      "Train on 32651035 samples, validate on 3627893 samples\n",
      "Epoch 1/1\n",
      "32651035/32651035 [==============================] - 23s - loss: 0.1093 - val_loss: 0.1092\n"
     ]
    }
   ],
   "source": [
    "#2 more training cycles with lower learning rate\n",
    "hist = retrain(epochs=[1,1], learns = [1e-2, 1e-3], batch_size=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3627893/3627893 [==============================] - 2s - loss: 0.1037     \n",
      "Epoch 2/2\n",
      "3627893/3627893 [==============================] - 2s - loss: 0.1026     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f090ba834e0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finally train on the validation data before submitting\n",
    "K.set_value(model.optimizer.lr, 1e-3)\n",
    "model.fit(map_valid, y_valid, batch_size=100000, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save weights and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wn = 'winning_combo_547.h5'\n",
    "# model.save_weights(simple+'weights/' + wn)\n",
    "model.load_weights(simple+'weights/' + wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'winning_with_sample_weight.csv.gz'\n",
    "preds = model.predict(map_test)\n",
    "sample_sub_df = pd.read_csv(slowdata + 'sample_submission.csv')\n",
    "sample_sub_df['unit_sales']=np.ndarray.flatten(denorm(preds, meany_train, stdevy_train)).clip(min=0)\n",
    "sample_sub_df.to_csv(results_path+ fname, index=False, compression='gzip')\n",
    "competition= 'favorita-grocery-sales-forecasting'\n",
    "res = results_path + fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kg submit -c $competition $res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
